# -*- coding: utf-8 -*-
"""Нагиев_practice5_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HboSf8cWXyfZls9MI8gfdTDwHBLKDJAF

# DQN

Задаем структуру аппроксимации $Q^\theta$, начальные вектор параметров $\theta$, вероятность исследования среды $\varepsilon = 1$.

Для каждого эпизода $k$ делаем:

Пока эпизод не закончен делаем:

- Находясь в состоянии $S_t$ совершаем действие $A_t \sim \pi(\cdot|S_t)$, где $\pi = \varepsilon\text{-greedy}(Q^\theta)$, получаем награду $R_t$  переходим в состояние $S_{t+1}$. Сохраняем $(S_t,A_t,R_t,S_{t+1}) \rightarrow Memory$


- Берем $\{(s_i,a_i,r_i,s'_i)\}_{i=1}^{n} \leftarrow Memory$, определяем целевые значения

$$
y_i =
\left\{
\begin{array}{ll}
r_i, &\text{ если } s'_i\text{ -терминальное},\\[0.0cm]
 r_i + \gamma \max\limits_{a'} Q^\theta(s'_i,a'), &\text{ иначе}
\end{array}
\right.
$$

функцию потерь $Loss(\theta) = \frac{1}{n}\sum\limits_{i=1}^n \big(y_i - Q^\theta(s_i,a_i)\big)^2$
и обновляем вектор параметров

$$
\theta \leftarrow \theta - \alpha \nabla_\theta Loss(\theta)
$$

- Уменьшаем $\varepsilon$
"""

import matplotlib.pyplot as plt
from tqdm import tqdm

def test_hyperparameters(state_dim, action_dim, hyperparameters, episodes, t_max, agent_classes):
    results = {agent_class.__name__: {} for agent_class in agent_classes}

    for params in tqdm(hyperparameters, desc="Hyperparameter Sets"):
        for agent_class in agent_classes:
            agent = agent_class(state_dim, action_dim, **params)
            rewards_per_episode = []

            for episode in range(episodes):
                total_reward = 0
                state = env.reset()
                for t in range(t_max):
                    action = agent.get_action(state)
                    next_state, reward, done, _ = env.step(action)
                    total_reward += reward
                    agent.fit(state, action, reward, done, next_state)
                    state = next_state
                    if done:
                        break

                rewards_per_episode.append(total_reward)

            results[agent_class.__name__][str(params)] = rewards_per_episode

    return results

def plot_combined_results(results):
    example_agent_class = next(iter(results.keys()))
    for params in results[example_agent_class]:
        plt.figure(figsize=(10, 6))
        for agent_class in results:
            if params in results[agent_class]:
                plt.plot(results[agent_class][params], label=f"{agent_class}")

        plt.title(f"Comparative Performance with Parameters: {params}")
        plt.xlabel("Episodes")
        plt.ylabel("Total Reward")
        plt.legend()
        plt.show()

import numpy as np
import random
import torch
import torch.nn as nn
import gym
class Qfunction(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.linear_1 = nn.Linear(state_dim, 64)
        self.linear_2 = nn.Linear(64, 64)
        self.linear_3 = nn.Linear(64, action_dim)
        self.activation = nn.ReLU()

    def forward(self, states):
        hidden = self.linear_1(states)
        hidden = self.activation(hidden)
        hidden = self.linear_2(hidden)
        hidden = self.activation(hidden)
        actions = self.linear_3(hidden)
        return actions

class DQN:
    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-3, batch_size=64, epsilon_decrease=0.01, epsilon_min=0.01):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.q_function = Qfunction(self.state_dim, self.action_dim)
        self.gamma = gamma
        self.batch_size = batch_size
        self.epsilon = 1
        self.epsilon_decrease = epsilon_decrease
        self.epsilon_min = epsilon_min
        self.memory = []
        self.optimzaer = torch.optim.Adam(self.q_function.parameters(), lr=lr)

    def get_action(self, state):
        q_values = self.q_function(torch.FloatTensor(state))
        argmax_action = torch.argmax(q_values)
        probs = self.epsilon * np.ones(self.action_dim) / self.action_dim
        probs[argmax_action] += 1 - self.epsilon
        action = np.random.choice(np.arange(self.action_dim), p=probs)
        return action

    def fit(self, state, action, reward, done, next_state):
        self.memory.append([state, action, reward, int(done), next_state])

        if len(self.memory) > self.batch_size:
            batch = random.sample(self.memory, self.batch_size)
            states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))

            targets = rewards + self.gamma * (1 - dones) * torch.max(self.q_function(next_states), dim=1).values
            q_values = self.q_function(states)[torch.arange(self.batch_size), actions]

            loss = torch.mean((q_values - targets.detach()) ** 2)
            loss.backward()
            self.optimzaer.step()
            self.optimzaer.zero_grad()

            if self.epsilon > self.epsilon_min:
                self.epsilon -= self.epsilon_decrease

class DQNHard:
    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-3, batch_size=64, epsilon_decrease=0.01, epsilon_min=0.01, update_freq=100):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.q_function = Qfunction(self.state_dim, self.action_dim)
        self.target_q_function = Qfunction(self.state_dim, self.action_dim)
        self.target_q_function.load_state_dict(self.q_function.state_dict())
        self.gamma = gamma
        self.batch_size = batch_size
        self.epsilon = 1
        self.epsilon_decrease = epsilon_decrease
        self.epsilon_min = epsilon_min
        self.memory = []
        self.optimzaer = torch.optim.Adam(self.q_function.parameters(), lr=lr)
        self.update_freq = update_freq
        self.step_counter = 0

    def get_action(self, state):
        q_values = self.q_function(torch.FloatTensor(state))
        argmax_action = torch.argmax(q_values)
        probs = self.epsilon * np.ones(self.action_dim) / self.action_dim
        probs[argmax_action] += 1 - self.epsilon
        action = np.random.choice(np.arange(self.action_dim), p=probs)
        return action

    def fit(self, state, action, reward, done, next_state):
        self.memory.append([state, action, reward, int(done), next_state])

        if len(self.memory) > self.batch_size:
            batch = random.sample(self.memory, self.batch_size)
            states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))

            targets = rewards + self.gamma * (1 - dones) * torch.max(self.target_q_function(next_states), dim=1).values
            q_values = self.q_function(states)[torch.arange(self.batch_size), actions]

            loss = torch.mean((q_values - targets.detach()) ** 2)
            loss.backward()
            self.optimzaer.step()
            self.optimzaer.zero_grad()

            if self.epsilon > self.epsilon_min:
              self.epsilon -= self.epsilon_decrease
            self.epsilon = max(self.epsilon - self.epsilon_decrease, self.epsilon_min)
            self.step_counter += 1
            if self.step_counter % self.update_freq == 0:
                self.target_q_function.load_state_dict(self.q_function.state_dict())

class DQNSoft:
    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-3, batch_size=64, epsilon_decrease=0.01, epsilon_min=0.01, tau=0.001):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.q_function = Qfunction(self.state_dim, self.action_dim)
        self.target_q_function = Qfunction(self.state_dim, self.action_dim)
        self.target_q_function.load_state_dict(self.q_function.state_dict())
        self.gamma = gamma
        self.batch_size = batch_size
        self.epsilon = 1
        self.epsilon_decrease = epsilon_decrease
        self.epsilon_min = epsilon_min
        self.memory = []
        self.optimzaer = torch.optim.Adam(self.q_function.parameters(), lr=lr)
        self.tau = tau

    def soft_update(self, target, source):
        for target_param, local_param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)

    def get_action(self, state):
        q_values = self.q_function(torch.FloatTensor(state))
        argmax_action = torch.argmax(q_values)
        probs = self.epsilon * np.ones(self.action_dim) / self.action_dim
        probs[argmax_action] += 1 - self.epsilon
        action = np.random.choice(np.arange(self.action_dim), p=probs)
        return action

    def fit(self, state, action, reward, done, next_state):
        self.memory.append([state, action, reward, int(done), next_state])

        if len(self.memory) > self.batch_size:
            batch = random.sample(self.memory, self.batch_size)
            states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))

            targets = rewards + self.gamma * (1 - dones) * torch.max(self.target_q_function(next_states), dim=1).values
            q_values = self.q_function(states)[torch.arange(self.batch_size), actions]

            loss = torch.mean((q_values - targets.detach()) ** 2)
            loss.backward()
            self.optimzaer.step()
            self.optimzaer.zero_grad()

            if self.epsilon > self.epsilon_min:
                self.epsilon -= self.epsilon_decrease
            self.epsilon = max(self.epsilon - self.epsilon_decrease, self.epsilon_min)
            self.soft_update(self.target_q_function, self.q_function)

class DQNDouble:
    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-3, batch_size=64, epsilon_decrease=0.01, epsilon_min=0.01, tau=0.005):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.batch_size = batch_size
        self.epsilon = 1.0
        self.epsilon_decrease = epsilon_decrease
        self.epsilon_min = epsilon_min
        self.tau = tau

        self.q_function = Qfunction(state_dim, action_dim)
        self.target_q_function = Qfunction(state_dim, action_dim)
        self.target_q_function.load_state_dict(self.q_function.state_dict())
        self.optimizer = torch.optim.Adam(self.q_function.parameters(), lr=lr)

        self.memory = []

    def get_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)
        else:
            state = torch.FloatTensor(state).unsqueeze(0)
            q_values = self.q_function(state)
            return torch.argmax(q_values, dim=1).item()

    def fit(self, state, action, reward, done, next_state):
        self.memory.append((state, action, reward, done, next_state))

        if len(self.memory) < self.batch_size:
            return

        transitions = random.sample(self.memory, self.batch_size)
        batch_state, batch_action, batch_reward, batch_done, batch_next_state = zip(*transitions)

        batch_state = torch.FloatTensor(batch_state)
        batch_action = torch.LongTensor(batch_action)
        batch_reward = torch.FloatTensor(batch_reward)
        batch_done = torch.FloatTensor(batch_done)
        batch_next_state = torch.FloatTensor(batch_next_state)

        current_q = self.q_function(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)

        next_actions = self.q_function(batch_next_state).detach().max(1)[1]
        max_next_q = self.target_q_function(batch_next_state).gather(1, next_actions.unsqueeze(1)).squeeze(1)
        expected_q = batch_reward + self.gamma * max_next_q * (1 - batch_done)
        loss = nn.MSELoss()(current_q, expected_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        self.soft_update()
        self.epsilon = max(self.epsilon - self.epsilon_decrease, self.epsilon_min)

    def soft_update(self):
        for target_param, local_param in zip(self.target_q_function.parameters(), self.q_function.parameters()):
            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)

gamma_variants = [0.95, 0.97, 0.99]
lr_variants = [1e-3, 1e-4]
batch_size_variants = [ 32, 64]
epsilon_decrease_variants = [0.01]
epsilon_min_variants = [0.01]

hyperparameter_sets = []
for gamma in gamma_variants:
    for lr in lr_variants:
        for batch_size in batch_size_variants:
            for epsilon_decrease in epsilon_decrease_variants:
                for epilon_min in epsilon_min_variants:
                    hyperparameter_sets.append({
                        'gamma': gamma,
                        'lr': lr,
                        'batch_size': batch_size,
                        'epsilon_decrease': epsilon_decrease,
                        'epilon_min': epilon_min
                    })

import gym
import numpy as np
from tqdm import tqdm


def test_dqn_double(env, agent, episodes, t_max):
    rewards_per_episode = []

    for episode in tqdm(range(episodes), desc="Episodes"):
        total_reward = 0
        state = env.reset()
        for t in range(t_max):
            action = agent.get_action(state)
            next_state, reward, done, _ = env.step(action)
            agent.fit(state, action, reward, done, next_state)

            total_reward += reward
            state = next_state

            if done:
                break

        rewards_per_episode.append(total_reward)
    mean_last_50_rewards = np.mean(rewards_per_episode[-50:]) if len(rewards_per_episode) >= 50 else np.mean(rewards_per_episode)
    print(f"Mean Reward of Last 50 Episodes: {mean_last_50_rewards}")
    return rewards_per_episode

env = gym.make('Acrobot-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

agent = DQNDouble(state_dim, action_dim,gamma = 0.999, lr=1e-3, epsilon_decrease=0.0025)

episodes = 200
t_max = 500
rewards = test_dqn_double(env, agent, episodes, t_max)

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(rewards)
plt.title("Rewards per Episode for DQNDouble on Acrobot-v1")
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.show()

def plot_combined_results(results):
    example_agent_class = next(iter(results.keys()))
    for params in results[example_agent_class]:
        plt.figure(figsize=(10, 6))
        for agent_class in results:
            if params in results[agent_class]:
                plt.plot(results[agent_class][params], label=f"{agent_class}")

        plt.title(f"Comparative Performance with Parameters: {params}")
        plt.xlabel("Episodes")
        plt.ylabel("Total Reward")
        plt.legend()
        plt.show()
plot_combined_results(results)

def calculate_mean_last_50(rewards):
    if len(rewards) >= 50:
        return np.mean(rewards[-50:])
    else:
        return np.mean(rewards)

def calculate_mean_reward(rewards):
    return np.mean(rewards) if rewards else 0

def generate_sorted_lists(results):
    sorted_by_mean_last_50 = {}
    sorted_by_mean_total = {}

    for agent_class in results:
        sorted_by_mean_last_50[agent_class] = []
        sorted_by_mean_total[agent_class] = []
        for params in results[agent_class]:
            rewards = results[agent_class][params]
            mean_last_50 = calculate_mean_last_50(rewards)
            mean_total = calculate_mean_reward(rewards)
            sorted_by_mean_last_50[agent_class].append((params, mean_last_50))
            sorted_by_mean_total[agent_class].append((params, mean_total))

        sorted_by_mean_last_50[agent_class].sort(key=lambda x: x[1], reverse=True)
        sorted_by_mean_total[agent_class].sort(key=lambda x: x[1], reverse=True)

    return sorted_by_mean_last_50, sorted_by_mean_total

sorted_by_mean_last_50, sorted_by_mean_total = generate_sorted_lists(results)

for agent_class in sorted_by_mean_last_50:
    print(f"\nAgent: {agent_class}")
    print("Sorted by Mean Reward (Last 50):")
    for params, mean_reward in sorted_by_mean_last_50[agent_class]:
        print(f"Parameters: {params}, Mean Reward (Last 50): {mean_reward}")

for agent_class in sorted_by_mean_total:
    print(f"\nAgent: {agent_class}")
    print("Sorted by Mean Reward (Total):")
    for params, mean_reward in sorted_by_mean_total[agent_class]:
        print(f"Parameters: {params}, Mean Reward (Total): {mean_reward}")

gamma_variants = [0.99]
lr_variants = [1e-3]
batch_size_variants = [128]
epsilon_decrease_variants = [0.01]
epsilon_min_variants = [0.01]

hyperparameter_sets = []
for gamma in gamma_variants:
    for lr in lr_variants:
        for batch_size in batch_size_variants:
            for epsilon_decrease in epsilon_decrease_variants:
                for epilon_min in epsilon_min_variants:
                    hyperparameter_sets.append({
                        'gamma': gamma,
                        'lr': lr,
                        'batch_size': batch_size,
                        'epsilon_decrease': epsilon_decrease,
                        'epsilon_min': epilon_min
                    })

env = gym.make('Acrobot-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n
episode_n = 200
t_max = 500

agent_classes = [DQN, DQNHard, DQNSoft, DQNDouble]

results = test_hyperparameters(state_dim, action_dim, hyperparameter_sets, episode_n, t_max, agent_classes)

plot_combined_results(results)

def calculate_mean_last_50(rewards):
    if len(rewards) >= 50:
        return np.mean(rewards[-50:])
    else:
        return np.mean(rewards)

def calculate_mean_reward(rewards):
    return np.mean(rewards) if rewards else 0

def generate_sorted_lists(results):
    sorted_by_mean_last_50 = {}
    sorted_by_mean_total = {}

    for agent_class in results:
        sorted_by_mean_last_50[agent_class] = []
        sorted_by_mean_total[agent_class] = []
        for params in results[agent_class]:
            rewards = results[agent_class][params]
            mean_last_50 = calculate_mean_last_50(rewards)
            mean_total = calculate_mean_reward(rewards)
            sorted_by_mean_last_50[agent_class].append((params, mean_last_50))
            sorted_by_mean_total[agent_class].append((params, mean_total))

        sorted_by_mean_last_50[agent_class].sort(key=lambda x: x[1], reverse=True)
        sorted_by_mean_total[agent_class].sort(key=lambda x: x[1], reverse=True)

    return sorted_by_mean_last_50, sorted_by_mean_total

sorted_by_mean_last_50, sorted_by_mean_total = generate_sorted_lists(results)

for agent_class in sorted_by_mean_last_50:
    print(f"\nAgent: {agent_class}")
    print("Sorted by Mean Reward (Last 50):")
    for params, mean_reward in sorted_by_mean_last_50[agent_class]:
        print(f"Parameters: {params}, Mean Reward (Last 50): {mean_reward}")

for agent_class in sorted_by_mean_total:
    print(f"\nAgent: {agent_class}")
    print("Sorted by Mean Reward (Total):")
    for params, mean_reward in sorted_by_mean_total[agent_class]:
        print(f"Parameters: {params}, Mean Reward (Total): {mean_reward}")

tau_variants = [0.01, 0.015, 0.005, 0.001]
gamma_variants = [0.99]
lr_variants = [1e-3]
batch_size_variants = [128]
epsilon_decrease_variants = [0.01,0.001]
epsilon_min_variants = [0.01,0.015,0.02]

soft_double_hyperparameters = []
for tau in tau_variants:
    for gamma in gamma_variants:
        for lr in lr_variants:
            for batch_size in batch_size_variants:
                for epsilon_decrease in epsilon_decrease_variants:
                    for epsilon_min in epsilon_min_variants:
                        soft_double_hyperparameters.append({
                            'tau': tau,
                            'gamma': gamma,
                            'lr': lr,
                            'batch_size': batch_size,
                            'epsilon_decrease': epsilon_decrease,
                            'epsilon_min': epsilon_min
                        })

update_freq_variants = [100, 200, 50]
hard_hyperparameters = []
for update_freq in update_freq_variants:
    for gamma in gamma_variants:
        for lr in lr_variants:
            for batch_size in batch_size_variants:
                for epsilon_decrease in epsilon_decrease_variants:
                    for epsilon_min in epsilon_min_variants:
                        hard_hyperparameters.append({
                            'update_freq': update_freq,
                            'gamma': gamma,
                            'lr': lr,
                            'batch_size': batch_size,
                            'epsilon_decrease': epsilon_decrease,
                            'epsilon_min': epsilon_min
                        })

env = gym.make('Acrobot-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n
episode_n = 200
t_max = 500

all_results = {}

soft_double_results = test_hyperparameters(state_dim, action_dim, soft_double_hyperparameters, episode_n, t_max, [DQNSoft, DQNDouble])

hard_results = test_hyperparameters(state_dim, action_dim, hard_hyperparameters, episode_n, t_max, [DQNHard])

all_results = {**soft_double_results, **hard_results}

for agent_class in all_results:
    for params, rewards in all_results[agent_class].items():
        plt.figure(figsize=(10, 6))
        plt.plot(rewards, label=f"{agent_class} {params}")
        plt.title(f"Performance of {agent_class} with Parameters: {params}")
        plt.xlabel("Episodes")
        plt.ylabel("Total Reward")
        plt.legend()
        plt.show()

def calculate_mean_last_50(rewards):
    return np.mean(rewards[-50:]) if len(rewards) >= 50 else np.mean(rewards)

def calculate_mean_reward(rewards):
    return np.mean(rewards) if rewards else 0

def summarize_results(results):
    for agent_class in results:
        print(f"\nResults for {agent_class}:")
        for params, rewards in results[agent_class].items():
            mean_last_50 = calculate_mean_last_50(rewards)
            mean_total = calculate_mean_reward(rewards)
            print(f"Parameters: {params}")
            print(f"Mean Reward (Last 50): {mean_last_50}")
            print(f"Mean Reward (Total): {mean_total}")

summarize_results(soft_double_results)
summarize_results(hard_results)

import pandas as pd

def calculate_correlations(results):
    data = []
    for agent_class, params_results in results.items():
        for params, rewards in params_results.items():
            mean_reward = np.mean(rewards[-50:])
            params_dict = eval(params)
            params_dict['mean_reward'] = mean_reward
            params_dict['agent_class'] = agent_class
            data.append(params_dict)

    df = pd.DataFrame(data)

    for col in df.columns:
        if col not in ['mean_reward', 'agent_class']:
            correlation = df['mean_reward'].corr(df[col])
            print(f"Correlation between {col} and mean_reward: {correlation}")

calculate_correlations({**soft_double_results})

def calculate_correlations(results):
    data = []
    for agent_class, params_results in results.items():
        for params, rewards in params_results.items():
            mean_reward = np.mean(rewards[-50:])
            params_dict = eval(params)
            params_dict['mean_reward'] = mean_reward
            params_dict['agent_class'] = agent_class
            data.append(params_dict)

    df = pd.DataFrame(data)

    for col in df.columns:
        if col not in ['mean_reward', 'agent_class']:
            correlation = df['mean_reward'].corr(df[col])
            print(f"Correlation between {col} and mean_reward: {correlation}")

calculate_correlations({**hard_results})

tau_variants = [0.01, 0.015, 0.005, 0.001]
gamma_variants = [0.99]
lr_variants = [1e-3]
batch_size_variants = [128]
epsilon_decrease_variants = [0.01,0.001,0.003]
epsilon_min_variants = [0.01,0.015,0.02]

soft_double_hyperparameters = []
for tau in tau_variants:
    for gamma in gamma_variants:
        for lr in lr_variants:
            for batch_size in batch_size_variants:
                for epsilon_decrease in epsilon_decrease_variants:
                    for epsilon_min in epsilon_min_variants:
                        soft_double_hyperparameters.append({
                            'tau': tau,
                            'gamma': gamma,
                            'lr': lr,
                            'batch_size': batch_size,
                            'epsilon_decrease': epsilon_decrease,
                            'epsilon_min': epsilon_min
                        })

soft_double_results2 = test_hyperparameters(state_dim, action_dim, soft_double_hyperparameters, episode_n, t_max, [DQNDouble])

all_results = {**soft_double_results2}

for agent_class in all_results:
    for params, rewards in all_results[agent_class].items():
        plt.figure(figsize=(10, 6))
        plt.plot(rewards, label=f"{agent_class} {params}")
        plt.title(f"Performance of {agent_class} with Parameters: {params}")
        plt.xlabel("Episodes")
        plt.ylabel("Total Reward")
        plt.legend()
        plt.show()

def calculate_mean_last_50(rewards):
    return np.mean(rewards[-50:]) if len(rewards) >= 50 else np.mean(rewards)

def calculate_mean_reward(rewards):
    return np.mean(rewards) if rewards else 0

def summarize_results(results):
    for agent_class in results:
        print(f"\nResults for {agent_class}:")
        for params, rewards in results[agent_class].items():
            mean_last_50 = calculate_mean_last_50(rewards)
            mean_total = calculate_mean_reward(rewards)
            print(f"Parameters: {params}")
            print(f"Mean Reward (Last 50): {mean_last_50}")
            print(f"Mean Reward (Total): {mean_total}")

summarize_results(soft_double_results2)

def calculate_correlations(results):
    data = []
    for agent_class, params_results in results.items():
        for params, rewards in params_results.items():
            mean_reward = np.mean(rewards[-50:])
            params_dict = eval(params)
            params_dict['mean_reward'] = mean_reward
            params_dict['agent_class'] = agent_class
            data.append(params_dict)

    df = pd.DataFrame(data)

    for col in df.columns:
        if col not in ['mean_reward', 'agent_class']:
            correlation = df['mean_reward'].corr(df[col])
            print(f"Correlation between {col} and mean_reward: {correlation}")

calculate_correlations({**soft_double_results2})