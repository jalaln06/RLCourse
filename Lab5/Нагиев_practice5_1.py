# -*- coding: utf-8 -*-
"""Нагиев_Practice5_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YxQaDOXexeU2atbcCEenteGoZg0BF83L

# DQN

Задаем структуру аппроксимации $Q^\theta$, начальные вектор параметров $\theta$, вероятность исследования среды $\varepsilon = 1$.

Для каждого эпизода $k$ делаем:

Пока эпизод не закончен делаем:

- Находясь в состоянии $S_t$ совершаем действие $A_t \sim \pi(\cdot|S_t)$, где $\pi = \varepsilon\text{-greedy}(Q^\theta)$, получаем награду $R_t$  переходим в состояние $S_{t+1}$. Сохраняем $(S_t,A_t,R_t,S_{t+1}) \rightarrow Memory$


- Берем $\{(s_i,a_i,r_i,s'_i)\}_{i=1}^{n} \leftarrow Memory$, определяем целевые значения

$$
y_i =
\left\{
\begin{array}{ll}
r_i, &\text{ если } s'_i\text{ -терминальное},\\[0.0cm]
 r_i + \gamma \max\limits_{a'} Q^\theta(s'_i,a'), &\text{ иначе}
\end{array}
\right.
$$

функцию потерь $Loss(\theta) = \frac{1}{n}\sum\limits_{i=1}^n \big(y_i - Q^\theta(s_i,a_i)\big)^2$
и обновляем вектор параметров

$$
\theta \leftarrow \theta - \alpha \nabla_\theta Loss(\theta)
$$

- Уменьшаем $\varepsilon$
"""

import numpy as np
import random
import torch
import torch.nn as nn

class Qfunction(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.linear_1 = nn.Linear(state_dim, 64)
        self.linear_2 = nn.Linear(64, 64)
        self.linear_3 = nn.Linear(64, action_dim)
        self.activation = nn.ReLU()

    def forward(self, states):
        hidden = self.linear_1(states)
        hidden = self.activation(hidden)
        hidden = self.linear_2(hidden)
        hidden = self.activation(hidden)
        actions = self.linear_3(hidden)
        return actions

class DQN:
    def __init__(self, state_dim, action_dim, gamma=0.99, lr=1e-3, batch_size=64, epsilon_decrease=0.01, epilon_min=0.01):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.q_function = Qfunction(self.state_dim, self.action_dim)
        self.gamma = gamma
        self.batch_size = batch_size
        self.epsilon = 1
        self.epsilon_decrease = epsilon_decrease
        self.epilon_min = epilon_min
        self.memory = []
        self.optimzaer = torch.optim.Adam(self.q_function.parameters(), lr=lr)

    def get_action(self, state):
        q_values = self.q_function(torch.FloatTensor(state))
        argmax_action = torch.argmax(q_values)
        probs = self.epsilon * np.ones(self.action_dim) / self.action_dim
        probs[argmax_action] += 1 - self.epsilon
        action = np.random.choice(np.arange(self.action_dim), p=probs)
        return action

    def fit(self, state, action, reward, done, next_state):
        self.memory.append([state, action, reward, int(done), next_state])

        if len(self.memory) > self.batch_size:
            batch = random.sample(self.memory, self.batch_size)
            states, actions, rewards, dones, next_states = map(torch.tensor, list(zip(*batch)))

            targets = rewards + self.gamma * (1 - dones) * torch.max(self.q_function(next_states), dim=1).values
            q_values = self.q_function(states)[torch.arange(self.batch_size), actions]

            loss = torch.mean((q_values - targets.detach()) ** 2)
            loss.backward()
            self.optimzaer.step()
            self.optimzaer.zero_grad()

            if self.epsilon > self.epilon_min:
                self.epsilon -= self.epsilon_decrease

import gym

gamma_variants = [0.95, 0.97, 0.99, 0.92]
lr_variants = [1e-2, 1e-3, 1e-4]
batch_size_variants = [ 32, 64]
epsilon_decrease_variants = [0.01]
epsilon_min_variants = [0.01]

hyperparameter_sets = []
for gamma in gamma_variants:
    for lr in lr_variants:
        for batch_size in batch_size_variants:
            for epsilon_decrease in epsilon_decrease_variants:
                for epilon_min in epsilon_min_variants:
                    hyperparameter_sets.append({
                        'gamma': gamma,
                        'lr': lr,
                        'batch_size': batch_size,
                        'epsilon_decrease': epsilon_decrease,
                        'epilon_min': epilon_min
                    })

import matplotlib.pyplot as plt
from tqdm import tqdm

def test_hyperparameters(state_dim, action_dim, hyperparameters, episodes, t_max):
    results = {}
    for params in tqdm(hyperparameters, desc="Hyperparameter Sets"):
        agent = DQN(state_dim, action_dim, **params)
        rewards_per_episode = []

        for episode in range(episodes):
            total_reward = 0
            state = env.reset()
            for t in range(t_max):
                action = agent.get_action(state)
                next_state, reward, done, _ = env.step(action)
                total_reward += reward
                agent.fit(state, action, reward, done, next_state)
                state = next_state
                if done:
                    break

            rewards_per_episode.append(total_reward)

        results[str(params)] = rewards_per_episode

    return results

def plot_results(results):
    for params, rewards in results.items():
        plt.figure(figsize=(10, 6))
        plt.plot(rewards)
        plt.title(f"Performance with Parameters: {params}")
        plt.xlabel("Episodes")
        plt.ylabel("Total Reward")
        plt.show()


env = gym.make('Acrobot-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n
episode_n = 200
t_max = 500

results = test_hyperparameters(state_dim, action_dim, hyperparameter_sets, episode_n, t_max)
plot_results(results)

sorted_by_mean_last_50, sorted_by_mean_total = generate_sorted_lists(results)

print("Sorted by Mean Reward (Last 50):")
for params, mean_reward in sorted_by_mean_last_50:
    print(f"Parameters: {params}, Mean Reward (Last 50): {mean_reward}")

print("\nSorted by Mean Reward (Total):")
for params, mean_reward in sorted_by_mean_total:
    print(f"Parameters: {params}, Mean Reward (Total): {mean_reward}")

gamma_variants = [0.99, 0.999,0.9999]
lr_variants = [1e-3, 1e-4]
batch_size_variants = [32, 64]
epsilon_decrease_variants = [0.01]
epsilon_min_variants = [0.01]

hyperparameter_sets = []
for gamma in gamma_variants:
    for lr in lr_variants:
        for batch_size in batch_size_variants:
            for epsilon_decrease in epsilon_decrease_variants:
                for epilon_min in epsilon_min_variants:
                    hyperparameter_sets.append({
                        'gamma': gamma,
                        'lr': lr,
                        'batch_size': batch_size,
                        'epsilon_decrease': epsilon_decrease,
                        'epilon_min': epilon_min
                    })

import matplotlib.pyplot as plt
from tqdm import tqdm

def test_hyperparameters(state_dim, action_dim, hyperparameters, episodes, t_max):
    results = {}
    for params in tqdm(hyperparameters, desc="Hyperparameter Sets"):
        agent = DQN(state_dim, action_dim, **params)
        rewards_per_episode = []

        for episode in range(episodes):
            total_reward = 0
            state = env.reset()
            for t in range(t_max):
                action = agent.get_action(state)
                next_state, reward, done, _ = env.step(action)
                total_reward += reward
                agent.fit(state, action, reward, done, next_state)
                state = next_state
                if done:
                    break

            rewards_per_episode.append(total_reward)

        results[str(params)] = rewards_per_episode

    return results

def plot_results(results):
    for params, rewards in results.items():
        plt.figure(figsize=(10, 6))
        plt.plot(rewards)
        plt.title(f"Performance with Parameters: {params}")
        plt.xlabel("Episodes")
        plt.ylabel("Total Reward")
        plt.show()


env = gym.make('Acrobot-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n
episode_n = 200
t_max = 500

results = test_hyperparameters(state_dim, action_dim, hyperparameter_sets, episode_n, t_max)
plot_results(results)

def calculate_mean_last_50(rewards):
    if len(rewards) >= 50:
        return np.mean(rewards[-50:])
    else:
        return np.mean(rewards)

def calculate_mean_reward(rewards):
    return np.mean(rewards) if rewards else 0

def generate_sorted_lists(results):
    sorted_by_mean_last_50 = []
    sorted_by_mean_total = []

    for params, rewards in results.items():
        mean_last_50 = calculate_mean_last_50(rewards)
        mean_total = calculate_mean_reward(rewards)
        sorted_by_mean_last_50.append((params, mean_last_50))
        sorted_by_mean_total.append((params, mean_total))

    sorted_by_mean_last_50.sort(key=lambda x: x[1], reverse=True)
    sorted_by_mean_total.sort(key=lambda x: x[1], reverse=True)

    return sorted_by_mean_last_50, sorted_by_mean_total

sorted_by_mean_last_50, sorted_by_mean_total = generate_sorted_lists(results)

print("Sorted by Mean Reward (Last 50):")
for params, mean_reward in sorted_by_mean_last_50:
    print(f"Parameters: {params}, Mean Reward (Last 50): {mean_reward}")

print("\nSorted by Mean Reward (Total):")
for params, mean_reward in sorted_by_mean_total:
    print(f"Parameters: {params}, Mean Reward (Total): {mean_reward}")

import matplotlib.pyplot as plt
from tqdm import tqdm

def test_hyperparameters(state_dim, action_dim, hyperparameters, episodes, t_max):
    results = {}
    for params in tqdm(hyperparameters, desc="Hyperparameter Sets"):
        agent = DQN(state_dim, action_dim, **params)
        rewards_per_episode = []

        for episode in range(episodes):
            total_reward = 0
            state = env.reset()
            for t in range(t_max):
                action = agent.get_action(state)
                next_state, reward, done, _ = env.step(action)
                total_reward += reward
                agent.fit(state, action, reward, done, next_state)
                state = next_state
                if done:
                    break

            rewards_per_episode.append(total_reward)

        results[str(params)] = rewards_per_episode

    return results

def plot_results(results):
    for params, rewards in results.items():
        plt.figure(figsize=(10, 6))
        plt.plot(rewards)
        plt.title(f"Performance with Parameters: {params}")
        plt.xlabel("Episodes")
        plt.ylabel("Total Reward")
        plt.show()


env = gym.make('Acrobot-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n
episode_n = 500
t_max = 500

results = test_hyperparameters(state_dim, action_dim, hyperparameter_sets, episode_n, t_max)
plot_results(results)

def calculate_mean_last_50(rewards):
    if len(rewards) >= 50:
        return np.mean(rewards[-50:])
    else:
        return np.mean(rewards)

def calculate_mean_reward(rewards):
    return np.mean(rewards) if rewards else 0

def generate_sorted_lists(results):
    sorted_by_mean_last_50 = []
    sorted_by_mean_total = []

    for params, rewards in results.items():
        mean_last_50 = calculate_mean_last_50(rewards)
        mean_total = calculate_mean_reward(rewards)
        sorted_by_mean_last_50.append((params, mean_last_50))
        sorted_by_mean_total.append((params, mean_total))

    sorted_by_mean_last_50.sort(key=lambda x: x[1], reverse=True)
    sorted_by_mean_total.sort(key=lambda x: x[1], reverse=True)

    return sorted_by_mean_last_50, sorted_by_mean_total

sorted_by_mean_last_50, sorted_by_mean_total = generate_sorted_lists(results)

print("Sorted by Mean Reward (Last 50):")
for params, mean_reward in sorted_by_mean_last_50:
    print(f"Parameters: {params}, Mean Reward (Last 50): {mean_reward}")

print("\nSorted by Mean Reward (Total):")
for params, mean_reward in sorted_by_mean_total:
    print(f"Parameters: {params}, Mean Reward (Total): {mean_reward}")

import torch
from torch import nn
import numpy as np
import gym
import matplotlib.pyplot as plt

class CEM(nn.Module):
    def __init__(self, state_dim, action_n):
        super().__init__()
        self.state_dim = state_dim
        self.action_n = action_n

        self.network = nn.Sequential(
            nn.Linear(self.state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, self.action_n)
        )

        self.softmax = nn.Softmax(dim=1)
        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)
        self.loss = nn.CrossEntropyLoss()

    def forward(self, _input):
        return self.network(_input)

    def get_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        logits = self.forward(state)
        action_prob = self.softmax(logits).detach().numpy().flatten()
        action = np.random.choice(self.action_n, p=action_prob)
        return action

    def update_policy(self, elite_trajectories):
        elite_states = []
        elite_actions = []
        for trajectory in elite_trajectories:
            elite_states.extend(trajectory['states'])
            elite_actions.extend(trajectory['actions'])
        elite_states = torch.FloatTensor(elite_states)
        elite_actions = torch.LongTensor(elite_actions)

        loss = self.loss(self.forward(elite_states), elite_actions)
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()

def get_trajectory(env, agent, trajectory_len, visualize=False):
    trajectory = {'states':[], 'actions': [], 'total_reward': 0}

    state = env.reset()
    trajectory['states'].append(state)

    for _ in range(trajectory_len):

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        state, reward, done, _ = env.step(action)
        trajectory['total_reward'] += reward

        if done:
            break

        if visualize:
            env.render()

        trajectory['states'].append(state)

    return trajectory

def get_elite_trajectories(trajectories, q_param):
    total_rewards = [trajectory['total_reward'] for trajectory in trajectories]
    quantile = np.quantile(total_rewards, q=q_param)
    return [trajectory for trajectory in trajectories if trajectory['total_reward'] > quantile]

env = gym.make('Acrobot-v1')
state_dim = env.observation_space.shape[0]
action_n = env.action_space.n

agent = CEM(state_dim, action_n)
episode_n = 100
trajectory_n = 200
trajectory_len = 500
q_param = 0.7
scores = []
for episode in range(episode_n):
    trajectories = [get_trajectory(env, agent, trajectory_len) for _ in range(trajectory_n)]

    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])
    print(f'episode: {episode}, mean_total_reward = {mean_total_reward}')
    scores.append(mean_total_reward)
    elite_trajectories = get_elite_trajectories(trajectories, q_param)

    if len(elite_trajectories) > 0:
        agent.update_policy(elite_trajectories)

res = get_trajectory(env, agent, trajectory_len, visualize=False)

fig = plt.figure()
ax = fig.add_subplot(111)
plt.plot(np.arange(1, len(scores)+1), scores)
plt.ylabel('Score')
plt.xlabel('Episode #')
plt.show()