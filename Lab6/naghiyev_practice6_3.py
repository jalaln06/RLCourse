# -*- coding: utf-8 -*-
"""Naghiyev_practice6_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bSGClPFhMna1MVFBy59PU8kZLjAQlJHp
"""

import gym as gym
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from torch.distributions import Categorical

class PPO(nn.Module):
    def __init__(self, state_dim, action_dim, gamma=0.99, batch_size=64, epsilon=0.2, epoch_n=20, pi_lr=1e-4, v_lr=5e-4):

        super().__init__()

        self.pi_model = nn.Sequential(nn.Linear(state_dim, 128), nn.ReLU(),
                                      nn.Linear(128, 128), nn.ReLU(),
                                      nn.Linear(128, action_dim), nn.Softmax(dim=-1))

        self.v_model = nn.Sequential(nn.Linear(state_dim, 128), nn.ReLU(),
                                     nn.Linear(128, 128), nn.ReLU(),
                                     nn.Linear(128, 1))
        self.gamma = gamma
        self.batch_size = batch_size
        self.epsilon = epsilon
        self.epoch_n = epoch_n
        self.pi_optimizer = torch.optim.Adam(self.pi_model.parameters(), lr=pi_lr)
        self.v_optimizer = torch.optim.Adam(self.v_model.parameters(), lr=v_lr)

    def get_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        probs = self.pi_model(state)
        dist = Categorical(probs)
        action = dist.sample()

        return action.item(), dist.log_prob(action)

    def fit(self, states, actions, rewards, dones):
        states, actions, rewards, dones = map(np.array, [states, actions, rewards, dones])
        rewards, dones = rewards.reshape(-1, 1), dones.reshape(-1, 1)

        returns = np.zeros(rewards.shape)
        returns[-1] = rewards[-1]
        for t in range(returns.shape[0] - 2, -1, -1):
            returns[t] = rewards[t] + (1 - dones[t]) * self.gamma * returns[t + 1]

        states, actions, returns = map(torch.FloatTensor, [states, actions, returns])
        actions = actions.long()

        probs = self.pi_model(states)
        dist = Categorical(probs)
        old_log_probs = dist.log_prob(actions).detach()

        for epoch in range(self.epoch_n):
            idxs = np.random.permutation(returns.shape[0])
            for i in range(0, returns.shape[0], self.batch_size):
                b_idxs = idxs[i: i + self.batch_size]
                b_states = states[b_idxs]
                b_actions = actions[b_idxs]
                b_returns = returns[b_idxs]
                b_old_log_probs = old_log_probs[b_idxs]

                b_advantage = b_returns.detach() - self.v_model(b_states)

                b_probs = self.pi_model(b_states)
                b_dist = Categorical(b_probs)
                b_new_log_probs = b_dist.log_prob(b_actions)


                b_ratio = torch.exp(b_new_log_probs - b_old_log_probs)
                pi_loss_1 = b_ratio * b_advantage.detach()
                pi_loss_2 = torch.clamp(b_ratio, 1. - self.epsilon, 1. + self.epsilon) * b_advantage.detach()
                pi_loss = - torch.mean(torch.min(pi_loss_1, pi_loss_2))
                self.pi_optimizer.zero_grad()
                pi_loss.backward()
                self.pi_optimizer.step()

                v_loss = torch.mean((b_advantage ** 2).squeeze())

                self.v_optimizer.zero_grad()
                v_loss.backward()
                self.v_optimizer.step()

import gym
import numpy as np
import matplotlib.pyplot as plt


env = gym.make('Acrobot-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

agent = PPO(state_dim, action_dim)

episode_n = 60
trajectory_n = 50

mean_rewards = []

for episode in range(episode_n):

    states, actions, rewards, train_rewards, dones = [], [], [], [], []
    total_reward = 0

    for _ in range(trajectory_n):
        state = env.reset()
        traj_rew = 0
        for t in range(200):
            states.append(state)

            action, _ = agent.get_action(state)
            actions.append(action)

            state, reward, done, _ = env.step(action)
            #if reward == 0:
                #train_reward = 100
               # train_rewards.append(train_reward)
           # else:
               # train_rewards.append(reward)
            rewards.append(reward)

            dones.append(done)
            traj_rew += reward
            total_reward += reward
            if done:
                break
    mean_reward = total_reward / trajectory_n
    mean_rewards.append(mean_reward)
    print(f"Episode {episode + 1}, Mean Reward: {mean_reward}")

    agent.fit(states, actions, rewards, dones)

plt.plot(mean_rewards)
plt.title('Mean Rewards per Episode')
plt.xlabel('Episode')
plt.ylabel('Mean Reward')
plt.grid()
plt.show()

episode_n=20
for episode in range(episode_n):

    states, actions, rewards, train_rewards, dones = [], [], [], [], []
    total_reward = 0

    for _ in range(trajectory_n):
        state = env.reset()
        traj_rew = 0
        for t in range(200):
            states.append(state)

            action, _ = agent.get_action(state)
            actions.append(action)

            state, reward, done, _ = env.step(action)
            #if reward == 0:
            #train_reward = 100
            # train_rewards.append(train_reward)
            # else:
            # train_rewards.append(reward)
            rewards.append(reward)

            dones.append(done)
            traj_rew += reward
            total_reward += reward
            if done:
                break
    mean_reward = total_reward / trajectory_n
    mean_rewards.append(mean_reward)
    print(f"Episode {episode + 1}, Mean Reward: {mean_reward}")

    agent.fit(states, actions, rewards, dones)

plt.plot(mean_rewards)
plt.title('Mean Rewards per Episode')
plt.xlabel('Episode')
plt.ylabel('Mean Reward')
plt.grid()
plt.show()

episode_n=20
for episode in range(episode_n):

    states, actions, rewards, train_rewards, dones = [], [], [], [], []
    total_reward = 0

    for _ in range(trajectory_n):
        state = env.reset()
        traj_rew = 0
        for t in range(200):
            states.append(state)

            action, _ = agent.get_action(state)
            actions.append(action)

            state, reward, done, _ = env.step(action)
            #if reward == 0:
            #train_reward = 100
            # train_rewards.append(train_reward)
            # else:
            # train_rewards.append(reward)
            rewards.append(reward)

            dones.append(done)
            traj_rew += reward
            total_reward += reward
            if done:
                break
    mean_reward = total_reward / trajectory_n
    mean_rewards.append(mean_reward)
    print(f"Episode {episode + 1}, Mean Reward: {mean_reward}")

    agent.fit(states, actions, rewards, dones)

plt.plot(mean_rewards)
plt.title('Mean Rewards per Episode')
plt.xlabel('Episode')
plt.ylabel('Mean Reward')
plt.grid()
plt.show()

episode_n=20
for episode in range(episode_n):

    states, actions, rewards, train_rewards, dones = [], [], [], [], []
    total_reward = 0

    for _ in range(trajectory_n):
        state = env.reset()
        traj_rew = 0
        for t in range(200):
            states.append(state)

            action, _ = agent.get_action(state)
            actions.append(action)

            state, reward, done, _ = env.step(action)
            #if reward == 0:
            #train_reward = 100
            # train_rewards.append(train_reward)
            # else:
            # train_rewards.append(reward)
            rewards.append(reward)

            dones.append(done)
            traj_rew += reward
            total_reward += reward
            if done:
                break
    mean_reward = total_reward / trajectory_n
    mean_rewards.append(mean_reward)
    print(f"Episode {episode + 1}, Mean Reward: {mean_reward}")

    agent.fit(states, actions, rewards, dones)

plt.plot(mean_rewards)
plt.title('Mean Rewards per Episode')
plt.xlabel('Episode')
plt.ylabel('Mean Reward')
plt.grid()
plt.show()

