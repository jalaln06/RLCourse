# -*- coding: utf-8 -*-
"""Нагиев_practice4_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13V6Xlns8tlIZAFU4cah6sS6n1ReWsGYw
"""

import torch
from torch import nn
import numpy as np
import gym
import matplotlib.pyplot as plt

class CEM(nn.Module):
    def __init__(self, state_dim, action_n):
        super().__init__()
        self.state_dim = state_dim
        self.action_n = action_n

        self.network = nn.Sequential(
            nn.Linear(self.state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, self.action_n)
        )

        self.softmax = nn.Softmax(dim=1)
        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)
        self.loss = nn.CrossEntropyLoss()

    def forward(self, _input):
        return self.network(_input)

    def get_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        logits = self.forward(state)
        action_prob = self.softmax(logits).detach().numpy().flatten()
        action = np.random.choice(self.action_n, p=action_prob)
        return action

    def update_policy(self, elite_trajectories):
        elite_states = []
        elite_actions = []
        for trajectory in elite_trajectories:
            elite_states.extend(trajectory['states'])
            elite_actions.extend(trajectory['actions'])
        elite_states = torch.FloatTensor(elite_states)
        elite_actions = torch.LongTensor(elite_actions)

        loss = self.loss(self.forward(elite_states), elite_actions)
        loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()

def get_trajectory(env, agent, trajectory_len, visualize=False):
    trajectory = {'states':[], 'actions': [], 'total_reward': 0}

    state = env.reset()
    trajectory['states'].append(state)

    for _ in range(trajectory_len):

        action = agent.get_action(state)
        trajectory['actions'].append(action)

        state, reward, done, _ = env.step(action)
        trajectory['total_reward'] += reward

        if done:
            break

        if visualize:
            env.render()

        trajectory['states'].append(state)

    return trajectory

def get_elite_trajectories(trajectories, q_param):
    total_rewards = [trajectory['total_reward'] for trajectory in trajectories]
    quantile = np.quantile(total_rewards, q=q_param)
    return [trajectory for trajectory in trajectories if trajectory['total_reward'] > quantile]

env = gym.make('Acrobot-v1')
state_dim = env.observation_space.shape[0]
action_n = env.action_space.n

agent = CEM(state_dim, action_n)
episode_n = 100
trajectory_n = 200
trajectory_len = 500
q_param = 0.7
scores = []
for episode in range(episode_n):
    trajectories = [get_trajectory(env, agent, trajectory_len) for _ in range(trajectory_n)]

    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])
    print(f'episode: {episode}, mean_total_reward = {mean_total_reward}')
    scores.append(mean_total_reward)
    elite_trajectories = get_elite_trajectories(trajectories, q_param)

    if len(elite_trajectories) > 0:
        agent.update_policy(elite_trajectories)

res = get_trajectory(env, agent, trajectory_len, visualize=False)

fig = plt.figure()
ax = fig.add_subplot(111)
plt.plot(np.arange(1, len(scores)+1), scores)
plt.ylabel('Score')
plt.xlabel('Episode #')
plt.show()

import numpy as np
import gym

def get_discrete_state_acrobot(state, bins):
    cos1_linspace = np.linspace(-1, 1, bins)
    sin1_linspace = np.linspace(-1, 1, bins)
    cos2_linspace = np.linspace(-1, 1, bins)
    sin2_linspace = np.linspace(-1, 1, bins)

    vel1_linspace = np.linspace(-12.567 , 12.567, bins)
    vel2_linspace = np.linspace(-28.274 , 28.274 , bins)

    cos1_idx = np.abs(cos1_linspace - state[0]).argmin()
    sin1_idx = np.abs(sin1_linspace - state[1]).argmin()
    cos2_idx = np.abs(cos2_linspace - state[2]).argmin()
    sin2_idx = np.abs(sin2_linspace - state[3]).argmin()
    vel1_idx = np.abs(vel1_linspace - state[4]).argmin()
    vel2_idx = np.abs(vel2_linspace - state[5]).argmin()

    return cos1_idx, sin1_idx, cos2_idx, sin2_idx, vel1_idx, vel2_idx
def get_epsilon_greedy_action(qfunction, state, epsilon, action_n):
    if np.random.random() < epsilon:
        return np.random.randint(action_n)
    else:
        return np.argmax(qfunction[state])

def MonteCarlo_Acrobot(env, episode_n, trajectory_len=500, gamma=0.99, bins=[10, 10, 10, 10, 10, 10]):
    total_rewards = []
    total_interactions = 0

    qfunction = np.zeros(tuple(bins + [env.action_space.n]))
    counter = np.zeros(tuple(bins + [env.action_space.n]))

    for episode in range(episode_n):
        epsilon = 1 - episode / episode_n
        trajectory = {'states': [], 'actions': [], 'rewards': []}

        state = discretize_state(env.reset(), bins)

        for _ in range(trajectory_len):
            trajectory['states'].append(state)

            action = get_epsilon_greedy_action(qfunction, state, epsilon, env.action_space.n)
            trajectory['actions'].append(action)

            next_state, reward, done, _ = env.step(action)
            next_state = discretize_state(next_state, bins)
            trajectory['rewards'].append(reward)
            total_interactions += 1

            if done:
                break

            state = next_state

        total_rewards.append(sum(trajectory['rewards']))

        real_trajectory_len = len(trajectory['rewards'])
        returns = np.zeros(real_trajectory_len + 1)
        for t in range(real_trajectory_len - 1, -1, -1):
            returns[t] = trajectory['rewards'][t] + gamma * returns[t + 1]

        for t in range(real_trajectory_len):
            state = trajectory['states'][t]
            action = trajectory['actions'][t]
            qfunction[state][action] += (returns[t] - qfunction[state][action]) / (1 + counter[state][action])
            counter[state][action] += 1

    return total_rewards, total_interactions

env = gym.make('Acrobot-v1')
episode_n = 100
trajectory_len = 500
gamma = 0.99
bins = [10, 10, 10, 10, 10, 10]

rewards, interactions = MonteCarlo_Acrobot(env, episode_n, trajectory_len, gamma, bins)

plt.plot(rewards)
plt.xlabel('Episodes')
plt.ylabel('Total Reward')
plt.title('Monte carlo Training Performance on Acrobot-v1')
plt.show()

import numpy as np
import gym

def get_discrete_state_acrobot(state, bins):
    cos1_linspace = np.linspace(-1, 1, bins)
    sin1_linspace = np.linspace(-1, 1, bins)
    cos2_linspace = np.linspace(-1, 1, bins)
    sin2_linspace = np.linspace(-1, 1, bins)

    vel1_linspace = np.linspace(-12.567 , 12.567, bins)
    vel2_linspace = np.linspace(-28.274 , 28.274 , bins)

    cos1_idx = np.abs(cos1_linspace - state[0]).argmin()
    sin1_idx = np.abs(sin1_linspace - state[1]).argmin()
    cos2_idx = np.abs(cos2_linspace - state[2]).argmin()
    sin2_idx = np.abs(sin2_linspace - state[3]).argmin()
    vel1_idx = np.abs(vel1_linspace - state[4]).argmin()
    vel2_idx = np.abs(vel2_linspace - state[5]).argmin()

    return cos1_idx, sin1_idx, cos2_idx, sin2_idx, vel1_idx, vel2_idx

def get_epsilon_greedy_action(qfunction, state, epsilon, action_n):
    if np.random.random() < epsilon:
        return np.random.randint(action_n)
    else:
        return np.argmax(qfunction[state])

def SARSA_Acrobot(env, episode_n, gamma=0.99, trajectory_len=500, alpha=0.5, bins=10):
    total_rewards = np.zeros(episode_n)
    action_n = env.action_space.n
    state_n = [bins] * 6
    qfunction = np.zeros(state_n + [action_n])

    for episode in range(episode_n):
        epsilon = 1 / (episode + 1)
        state = get_discrete_state_acrobot(env.reset(), bins)
        action = get_epsilon_greedy_action(qfunction, state, epsilon, action_n)

        for _ in range(trajectory_len):
            next_state, reward, done, _ = env.step(action)
            next_state = get_discrete_state_acrobot(next_state, bins)
            next_action = get_epsilon_greedy_action(qfunction, next_state, epsilon, action_n)

            qfunction[state][action] += alpha * (reward + gamma * qfunction[next_state][next_action] - qfunction[state][action])

            state, action = next_state, next_action
            total_rewards[episode] += reward

            if done:
                break

    return total_rewards

env = gym.make('Acrobot-v1')
episode_n = 1000
rewards = SARSA_Acrobot(env, episode_n)
env.close()

plt.plot(rewards)
plt.xlabel('Episodes')
plt.ylabel('Total Reward')
plt.title('SARSA Training Performance on Acrobot-v1')
plt.show()

def get_discrete_state_acrobot(state, bins):
    cos1_linspace = np.linspace(-1, 1, bins)
    sin1_linspace = np.linspace(-1, 1, bins)
    cos2_linspace = np.linspace(-1, 1, bins)
    sin2_linspace = np.linspace(-1, 1, bins)

    vel1_linspace = np.linspace(-12.567 , 12.567, bins)
    vel2_linspace = np.linspace(-28.274 , 28.274 , bins)

    cos1_idx = np.abs(cos1_linspace - state[0]).argmin()
    sin1_idx = np.abs(sin1_linspace - state[1]).argmin()
    cos2_idx = np.abs(cos2_linspace - state[2]).argmin()
    sin2_idx = np.abs(sin2_linspace - state[3]).argmin()
    vel1_idx = np.abs(vel1_linspace - state[4]).argmin()
    vel2_idx = np.abs(vel2_linspace - state[5]).argmin()

    return cos1_idx, sin1_idx, cos2_idx, sin2_idx, vel1_idx, vel2_idx
def QLearning_Acrobot(env, episode_n, gamma=0.99, trajectory_len=500, alpha=0.5, bins=10):
    total_rewards = np.zeros(episode_n)
    env_counter = 0

    action_n = env.action_space.n
    qfunction = np.zeros((bins, bins, bins, bins, bins, bins, action_n))

    for episode in range(episode_n):
        epsilon = 1 / (episode + 1)

        state = env.reset()
        discretized_state = get_discrete_state_acrobot(state, bins)
        env_counter += 1
        action = get_epsilon_greedy_action(qfunction, discretized_state, epsilon, action_n)

        for _ in range(trajectory_len):
            next_state, reward, done, _ = env.step(action)
            next_discretized_state = get_discrete_state_acrobot(next_state, bins)
            env_counter += 1

            best_next_action = np.argmax(qfunction[next_discretized_state])
            qfunction[discretized_state][action] += alpha * (reward + gamma * qfunction[next_discretized_state][best_next_action] - qfunction[discretized_state][action])

            discretized_state = next_discretized_state
            action = get_epsilon_greedy_action(qfunction, discretized_state, epsilon, action_n)

            total_rewards[episode] += reward

            if done:
                break

    return total_rewards, env_counter
env = gym.make('Acrobot-v1')

episode_n = 2000
gamma = 0.99
trajectory_len = 500
alpha = 0.1
bins = 10

total_rewards, _ = QLearning_Acrobot(env, episode_n, gamma, trajectory_len, alpha, bins)

import matplotlib.pyplot as plt

# Plot the rewards
plt.plot(total_rewards)
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.title('Q-Learning Training Performance on Acrobot-v1')
plt.show()