# -*- coding: utf-8 -*-
"""Нагиев_Practice4_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TLkeeaiVOM3yBTyrhIoE1y_Wc_C-V8DJ

### $\varepsilon$-Greedy Policy:
$$
\begin{array}{l}
\pi(a|s) =
\left\{
\begin{array}{ll}
1 - \varepsilon + \varepsilon / m,& \text{ если } a \in \mathrm{argmax}_{a' \in \mathcal{A}}\, Q(s,a'),\\
\varepsilon / m,& \text{ иначе }
\end{array}
\right.
\end{array}
$$
"""

import numpy as np


def get_epsilon_greedy_action(q_values, epsilon, action_n):
    policy = np.ones(action_n) * epsilon / action_n
    max_action = np.argmax(q_values)
    policy[max_action] += 1 - epsilon
    return np.random.choice(np.arange(action_n), p=policy)

"""### Monte-Carlo Algorithm

Пусть $Q(s,a) = 0$, $N(s,a) = 0$ и $\varepsilon = 1$.

Для каждого эпизода $k \in \overline{1,K}$ делаем:

1. Согласно $\pi = \varepsilon\text{-greedy}(Q)$ получаем траекторию $\tau = (S_0,A_0,\ldots,S_T)$ и награды $(R_0,\ldots,R_{T-1})$. По ним определяем $(G_0,\ldots,G_{T-1}):$
$$
G_t = \sum\limits_{k=t}^{T-1} \gamma^{k-t} R_t,\quad G_{T-1} = R_{T-1},\quad G_{T-2} = R_{T-2} + \gamma R_{T-1},\quad G_i = R_i + \gamma G_{i+1},\quad G_{T} = Q(S_T,\pi_{greedy}(S_T)).
$$

2. Для каждого $t \in \overline{0,T-1}$ обновляем $Q$ и $N$:

$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \frac{1}{N(S_t,A_t) + 1}\big(G_t - Q(S_t,A_t)\big),
$$

$$
N(S_t,A_t) \leftarrow N(S_t,A_t) + 1
$$
Уменьшаем $\varepsilon$

"""

def MonteCarlo(env, episode_n, trajectory_len=500, gamma=0.99):
    total_rewards = []
    total_interactions = 0
    interactions_history = []

    state_n = env.observation_space.n
    action_n = env.action_space.n
    qfunction = np.zeros((state_n, action_n))
    counter = np.zeros((state_n, action_n))

    for episode in range(episode_n):
        epsilon = 1 - episode / episode_n
        trajectory = {'states': [], 'actions': [], 'rewards': []}

        state = env.reset()

        for _ in range(trajectory_len):
            trajectory['states'].append(state)

            action = get_epsilon_greedy_action(qfunction[state], epsilon, action_n)
            trajectory['actions'].append(action)

            state, reward, done, _ = env.step(action)
            trajectory['rewards'].append(reward)
            total_interactions += 1

            if done:
                break

        total_rewards.append(sum(trajectory['rewards']))

        real_trajectory_len = len(trajectory['rewards'])
        returns = np.zeros(real_trajectory_len + 1)
        for t in range(real_trajectory_len - 1, -1, -1):
            returns[t] = trajectory['rewards'][t] + gamma * returns[t + 1]

        for t in range(real_trajectory_len):
            state = trajectory['states'][t]
            action = trajectory['actions'][t]
            qfunction[state][action] += (returns[t] - qfunction[state][action]) / (1 + counter[state][action])
            counter[state][action] += 1

        interactions_history.append(total_interactions)

    return total_rewards, interactions_history

import numpy as np
import gym

def MonteCarlo(env, episode_n, trajectory_len=500, gamma=0.99, static_epsilon=0.1):
    total_rewards = []
    total_interactions = 0
    interactions_history = []

    state_n = env.observation_space.n
    action_n = env.action_space.n
    qfunction = np.zeros((state_n, action_n))
    counter = np.zeros((state_n, action_n))

    for episode in range(episode_n):
        trajectory = {'states': [], 'actions': [], 'rewards': []}

        state = env.reset()

        for _ in range(trajectory_len):
            trajectory['states'].append(state)


            action = get_epsilon_greedy_action(qfunction[state], static_epsilon, action_n)
            trajectory['actions'].append(action)

            state, reward, done, _ = env.step(action)
            trajectory['rewards'].append(reward)
            total_interactions += 1

            if done:
                break

        total_rewards.append(sum(trajectory['rewards']))

        real_trajectory_len = len(trajectory['rewards'])
        returns = np.zeros(real_trajectory_len + 1)
        for t in range(real_trajectory_len - 1, -1, -1):
            returns[t] = trajectory['rewards'][t] + gamma * returns[t + 1]

        for t in range(real_trajectory_len):
            state = trajectory['states'][t]
            action = trajectory['actions'][t]
            qfunction[state][action] += (returns[t] - qfunction[state][action]) / (1 + counter[state][action])
            counter[state][action] += 1

        interactions_history.append(total_interactions)

    return total_rewards, interactions_history


def get_epsilon_greedy_action(q_values, epsilon, action_n):
    policy = np.ones(action_n) * epsilon / action_n
    max_action = np.argmax(q_values)
    policy[max_action] += 1 - epsilon
    return np.random.choice(np.arange(action_n), p=policy)


env = gym.make('Taxi-v3')
episode_n = 1000
total_rewards_static, interactions_history_static = MonteCarlo(env, episode_n, static_epsilon=0.1)

import numpy as np
import gym

def MonteCarlo(env, episode_n, trajectory_len=500, gamma=0.99, start_epsilon=1.0, end_epsilon=0.01, decay_rate=0.995):
    total_rewards = []
    total_interactions = 0
    interactions_history = []

    state_n = env.observation_space.n
    action_n = env.action_space.n
    qfunction = np.zeros((state_n, action_n))
    counter = np.zeros((state_n, action_n))

    epsilon = start_epsilon

    for episode in range(episode_n):
        trajectory = {'states': [], 'actions': [], 'rewards': []}

        state = env.reset()

        for _ in range(trajectory_len):
            trajectory['states'].append(state)

            action = get_epsilon_greedy_action(qfunction[state], epsilon, action_n)
            trajectory['actions'].append(action)

            state, reward, done, _ = env.step(action)
            trajectory['rewards'].append(reward)
            total_interactions += 1

            if done:
                break

        total_rewards.append(sum(trajectory['rewards']))


        real_trajectory_len = len(trajectory['rewards'])
        returns = np.zeros(real_trajectory_len + 1)
        for t in range(real_trajectory_len - 1, -1, -1):
            returns[t] = trajectory['rewards'][t] + gamma * returns[t + 1]


        for t in range(real_trajectory_len):
            state = trajectory['states'][t]
            action = trajectory['actions'][t]
            qfunction[state][action] += (returns[t] - qfunction[state][action]) / (1 + counter[state][action])
            counter[state][action] += 1

        interactions_history.append(total_interactions)


        epsilon = max(end_epsilon, epsilon * decay_rate)

    return total_rewards, interactions_history


def get_epsilon_greedy_action(q_values, epsilon, action_n):
    policy = np.ones(action_n) * epsilon / action_n
    max_action = np.argmax(q_values)
    policy[max_action] += 1 - epsilon
    return np.random.choice(np.arange(action_n), p=policy)


env = gym.make('Taxi-v3')
episode_n = 1000
total_rewards_decay, interactions_history_decay = MonteCarlo(env, episode_n)

def MonteCarlo(env, episode_n, trajectory_len=500, gamma=0.99, start_epsilon=1.0, end_epsilon=0.01, adaptation_factor=0.05):
    total_rewards = []
    total_interactions = 0
    interactions_history = []

    state_n = env.observation_space.n
    action_n = env.action_space.n
    qfunction = np.zeros((state_n, action_n))
    counter = np.zeros((state_n, action_n))

    epsilon = start_epsilon

    for episode in range(episode_n):
        trajectory = {'states': [], 'actions': [], 'rewards': []}

        state = env.reset()

        for _ in range(trajectory_len):
            trajectory['states'].append(state)

            action = get_epsilon_greedy_action(qfunction[state], epsilon, action_n)
            trajectory['actions'].append(action)

            state, reward, done, _ = env.step(action)
            trajectory['rewards'].append(reward)
            total_interactions += 1

            if done:
                break

        episode_reward = sum(trajectory['rewards'])
        total_rewards.append(episode_reward)


        real_trajectory_len = len(trajectory['rewards'])
        returns = np.zeros(real_trajectory_len + 1)
        for t in range(real_trajectory_len - 1, -1, -1):
            returns[t] = trajectory['rewards'][t] + gamma * returns[t + 1]


        for t in range(real_trajectory_len):
            state = trajectory['states'][t]
            action = trajectory['actions'][t]
            qfunction[state][action] += (returns[t] - qfunction[state][action]) / (1 + counter[state][action])
            counter[state][action] += 1

        interactions_history.append(total_interactions)


        avg_reward = np.mean(total_rewards[-100:])
        epsilon = max(end_epsilon, epsilon * (1 - adaptation_factor * avg_reward / max(total_rewards)))

    return total_rewards, interactions_history


def get_epsilon_greedy_action(q_values, epsilon, action_n):
    policy = np.ones(action_n) * epsilon / action_n
    max_action = np.argmax(q_values)
    policy[max_action] += 1 - epsilon
    return np.random.choice(np.arange(action_n), p=policy)


env = gym.make('Taxi-v3')
episode_n = 1000
total_rewards_adaptive, interactions_history_adaptive = MonteCarlo(env, episode_n)

def epsilon_uncertainty_based(qfunction, state, base_epsilon=0.1, uncertainty_factor=1.0):
    """
    Calculate epsilon based on the uncertainty in Q-values at a given state.
    Higher variance in Q-values will result in higher epsilon (more exploration).
    """
    q_values = qfunction[state]
    q_variance = np.var(q_values)
    epsilon = base_epsilon + uncertainty_factor * q_variance
    return min(1.0, max(0.0, epsilon))

def MonteCarlo(env, episode_n, trajectory_len=500, gamma=0.99, base_epsilon=0.1, uncertainty_factor=1.0):
    total_rewards = []
    total_interactions = 0
    interactions_history = []

    state_n = env.observation_space.n
    action_n = env.action_space.n
    qfunction = np.zeros((state_n, action_n))
    counter = np.zeros((state_n, action_n))

    for episode in range(episode_n):
        trajectory = {'states': [], 'actions': [], 'rewards': []}
        state = env.reset()

        for _ in range(trajectory_len):

            trajectory['states'].append(state)
            epsilon = epsilon_uncertainty_based(qfunction, state, base_epsilon, uncertainty_factor)
            action = get_epsilon_greedy_action(qfunction[state], epsilon, action_n)
            trajectory['actions'].append(action)


            next_state, reward, done, _ = env.step(action)
            trajectory['rewards'].append(reward)
            total_interactions += 1


            state = next_state

            if done:
                break


        episode_reward = sum(trajectory['rewards'])
        total_rewards.append(episode_reward)
        interactions_history.append(total_interactions)


        real_trajectory_len = len(trajectory['states'])
        returns = np.zeros(real_trajectory_len)
        for t in range(real_trajectory_len - 1, -1, -1):
            returns[t] = trajectory['rewards'][t] + (gamma * returns[t + 1] if t < real_trajectory_len - 1 else 0)
            state = trajectory['states'][t]
            action = trajectory['actions'][t]
            qfunction[state][action] += (returns[t] - qfunction[state][action]) / (1 + counter[state][action])
            counter[state][action] += 1

    return total_rewards, interactions_history


env = gym.make('Taxi-v3')
episode_n = 1000
total_rewards_action_value, interactions_history_action_value = MonteCarlo(env, episode_n)

def epsilon_decay_with_reset(episode, reset_interval=100, start_epsilon=1.0, end_epsilon=0.01, decay_rate=0.001):

    if episode % reset_interval == 0:
        return start_epsilon


    decayed_epsilon = max(end_epsilon, start_epsilon - decay_rate * (episode % reset_interval))
    return decayed_epsilon


def MonteCarlo(env, episode_n, trajectory_len=500, gamma=0.99, epsilon_schedule=epsilon_decay_with_reset):
    total_rewards = []
    total_interactions = 0
    interactions_history = []

    state_n = env.observation_space.n
    action_n = env.action_space.n
    qfunction = np.zeros((state_n, action_n))
    counter = np.zeros((state_n, action_n))

    for episode in range(episode_n):
        epsilon = epsilon_schedule(episode)
        trajectory = {'states': [], 'actions': [], 'rewards': []}

        state = env.reset()

        for _ in range(trajectory_len):
            action = get_epsilon_greedy_action(qfunction[state], epsilon, action_n)
            next_state, reward, done, _ = env.step(action)
            trajectory['states'].append(state)
            trajectory['actions'].append(action)
            trajectory['rewards'].append(reward)
            total_interactions += 1

            if done:
                break

            state = next_state

        episode_reward = sum(trajectory['rewards'])
        total_rewards.append(episode_reward)
        interactions_history.append(total_interactions)


        real_trajectory_len = len(trajectory['rewards'])
        returns = np.zeros(real_trajectory_len + 1)
        for t in range(real_trajectory_len - 1, -1, -1):
            returns[t] = trajectory['rewards'][t] + gamma * returns[t + 1]

        for t in range(real_trajectory_len):
            state = trajectory['states'][t]
            action = trajectory['actions'][t]
            qfunction[state][action] += (returns[t] - qfunction[state][action]) / (1 + counter[state][action])
            counter[state][action] += 1

    return total_rewards, interactions_history

env = gym.make('Taxi-v3')
episode_n = 1000

total_rewards_decay_with_reset, interactions_history_decay_with_reset = MonteCarlo(env, episode_n, epsilon_schedule=epsilon_decay_with_reset)

plt.figure(figsize=(12, 6))


plt.plot(total_rewards_static, label='Static')
plt.plot(total_rewards_decay, label='Decay')
plt.plot(total_rewards_adaptive, label='Adaptive')
plt.plot(total_rewards_action_value,label='Action Value')
plt.plot(total_rewards_decay_with_reset,label='Decay with reset')

plt.xlabel('Episodes')
plt.ylabel('Total Rewards')
plt.title('Total Rewards per Episode for Different Algorithms')
plt.legend()
plt.grid(True)
plt.show()